\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

\usepackage{placeins}

\usepackage{cite}

\usepackage[hidelinks]{hyperref}

\usepackage{amsmath,amssymb,amsfonts}

\usepackage{algorithmic}

\usepackage{graphicx}

\usepackage{textcomp}

\usepackage{xcolor}

\usepackage{tabularx}

\usepackage{ragged2e}

\usepackage{array}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{booktabs}

\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em

T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{DocuChat: A Retrieval-Augmented Document Question-Answering Assistant for PDF-Centric Workflows}

\author{
\IEEEauthorblockN{Doruk Kagan Ergin}
\IEEEauthorblockA{
\textit{Wydział Zarządzania i Nauk Technicznych} \\
\textit{Menedżerska Akademia Nauk Stosowanych w Warszawie} \\
Mazowieckie, Polska \\
78741@office.mans.org.pl
}
\and
\IEEEauthorblockN{Cuneyt Emre Durak}
\IEEEauthorblockA{
\textit{Wydział Zarządzania i Nauk Technicznych} \\
\textit{Menedżerska Akademia Nauk Stosowanych w Warszawie} \\
Mazowieckie, Polska \\
78745@office.mans.org.pl
}
\and
\IEEEauthorblockN{Emre Tamer}
\IEEEauthorblockA{
\textit{Wydział Zarządzania i Nauk Technicznych} \\
\textit{Menedżerska Akademia Nauk Stosowanych w Warszawie} \\
Mazowieckie, Polska \\
78713@office.mans.org.pl
}
\and
\IEEEauthorblockN{Alihan Karaca}
\IEEEauthorblockA{
\textit{Wydział Zarządzania i Nauk Technicznych} \\
\textit{Menedżerska Akademia Nauk Stosowanych w Warszawie} \\
Mazowieckie, Polska \\
78698@office.mans.org.pl
}
\and
\IEEEauthorblockN{Kumar Nalinaksh}
\IEEEauthorblockA{
\textit{Wydział Zarządzania i Nauk Technicznych} \\
\textit{Menedżerska Akademia Nauk Stosowanych w Warszawie} \\
Mazowieckie, Polska \\
kumar.nalinaksh@office.mans.org.pl
}
}

\maketitle

\begin{abstract}

Long form Pdf document have now become the norm in education research and industry but accessing specific information in them is still time consuming when navigation relies on keyword search and manual reading. Large language model enhance fluency in summarization and question answering whereas standalone generation is limited by the inability to access private documents and by hallucination that are not easily audited. 

This paper proposes DocuChat Pdf based question answering system that uses retrieval augmented generation to ground responses in user supplied documents DocuChat extract and cleans Pdf text divide it into overlapping chunk embeds the chunks into a shared vector space and retrieveal latency trade-offs and evidence-oriented interaction design.

\end{abstract}

\begin{IEEEkeywords}

Retrieval augmented generation, document question answering, Pdf processing dense retrieval FAISS and evidence grounding.

\end{IEEEkeywords}

\section{Introduction}\label{sec:introduction}

Technical reading required of students and professional has great increased in volume as scientific publications manuals and regulatory text continue to expand in size and revision frequency. Pdf remains a popular delivery format however its structure isnt always convenient for question driven access even when the required information exists in a document the coresponding sentence may be located several pages away from matching keyword.

Keywords search is useful baseline for many tasks but its limited by vocabulary mismatch and a shallow model of context. A query written using synonyms paraphrases or domain specific phrasing may fail to retrieve the relevant paragraph even when the answer is present. Furthermore a number of questions demand synthesis across multiple sections which isnt effectifely supported by single hit keyword matches.

Llms shift document interaction toward natural language question and conversational responses. Two practical problems remain. First a general purpose Llm cant answer questions about a private Pdf unless the document content is provided to the model. Second generation without explicit supporting evidence may produce answers that sound credible but lack documentation which is unacceptable in academic and complince oriented applications.

In this paper these shortcoming are addressed through the introduction of a RAG pdf question answering assistant called \emph{DocuChat}. The system retrieves relevant passages from the users document and conditions answer generation on that evidence prioritizing traceability over purely fluently output.

The objectives of this work are: to develop an end to end pdfd ingestion and retrieval pipeline suitable for interactive use and to expose evidence in interface to support verification. The main contributions of this paper are a complete RAG workflow for Pdf question answering including ingestion cleaning chunking dense retrieval and evidence conditioned response generation, an interface design that emphasizes auditability through retrieved context display and Pdf excerpt highlighting, and an empirical summary of retrieval accuracy user feedback and latency trends as retrieval depth varies clarifying the performance trade offs of topk retrieval.Paper organization: Section~\ref{sec:literature_review} reviews related approaches to document retrieval and RAG, Section~\ref{sec:challenges} summarize practical challenges for Pdf based Qa ~\ref{sec:implementation} describe the proposed solution and implementation Section~\ref{sec:results} consolidates evaluations outcomes into a unified result section and Sections~\ref{sec:future_work} and~\ref{sec:conclusion} discus extensions conclude the paper.

\section{Literature Review}

\label{sec:literature_review}

People often use old method and ask question from Pdf files or to summarize document this kind of task is called long document understanding in many system traditional information retrieval methods are used methods like TF IDFand BM25 are very popular because they are fast and easy to use ~\cite{SaltonBuckley1988},~\cite{RobertsonZaragoza2009} These method work well when the users question and the document use the same words.But problem start when question use different word.For example document say car but user ask vehicle old system can not understand this.It only match word not meanings becouse of this the system sometimes cannot find the correct answer even if it is present in the document this make system look blind.Another big problem is when answer is not in one place.In many pdf information is spread in many sentence or section.Old retrival method not good in this case.To fix this problem new system use neural retrival method.This method change question and document text into number these number is call vector.~\cite{Vaswani2017} Vector try show meaning of text so system not only see word but also idea behind word.Transformer modelhelp a lot for this these model read sentence with context. They look at the words to the left and the right to understanding the meaning ~\cite{Devlin2019} ~\cite{Johnson2017} dpr use two encoders the one encoder for question one encoder for passages both questions and passage go to same vector space.Because of this system can compare meaning not only word ~\cite{Karpukhin2020} .

in many test, dpr work much better than old sparse method.

But dense retrival also have problem.There can be million of vector. Searching all of them is slow for interactive document assistant and speed is very important user do not want wait long time sosystem use special tool like FAISS help search many vector very fast using Gpu ~\cite{Lewis2020RAG} but finding text is still not enough. User want direct answer or short summary. Because of this system also use text generation model.These model can write answer or summary from text ~\cite{Guu2020REALM}  ~\cite{Borgeaud2022RETRO} . In summarization task ROUGE score is often use ROUGE check how similar generate summary is to correct summary ~\cite{Asai2023SelfRAG} buttext generation model have big problem sometime they write thing that is not true. They can make up information.This is very bad in document system.User want answer that they can check in document.To fix this problem RAG method is use.In RAG system first search document.Then system generate answer only from found text.so system is not free to imagine.It must use evidence ~\cite{Beltagy2020Longformer} .This make answer more correct and less fake.Other system like REALM and RETRO also use retrival inside model.They help the model acquire knowledge from documents or databases ~\cite{Zaheer2020BigBird} ,~\cite{NogueiraCho2019} New ideas like Self RAG try to make the system smarter.System learn when need search and when not it also learn how fixed mistake by itself ~\cite{KhattabZaharia2020ColBERT}.These idea helps make better to Pdf assistant good Pdf assistant not only a give answer it also show where answer come from. For example system show page number or paragraph. So user can open Pdf and check.This make user trust system more. System performance depend on many thing. One important thing is a chunking. Chunking the means cutting document into small part. If chunk is too big system miss detail. If chunk is too small system become slow. Overlap between chunk is also important. Some overlap help keep meaning.

Another important factor retrieval depth which refer how many chunk the system select system usually pick the top K chunks alarger K provide more information but slows down the response, while a smaller K is faster but may miss the answer some systems use crossencoders like BERT to re rank results~\cite{KhattabZaharia2020ColBERT} which improve accuracy but requires more computation ColBERT aims achieve similar quality at a lower cost~\cite{Lewis2020BART}.Long documents are also challenging for standard transformers because they have limited context windows models such as longformer and BigBird address this by using special attention mechanisms to handle longer texts~\cite{Zhang2020PEGASUS,Lin2004ROUGE} even with longcontext models grounding and source visibility remain important as a user still want to see where answers come from ingeneral effective Pdf question answering and summarization system need strong retrieval find the relevant sections quickly efficient indexing and search and answer based on real evidence with clear source these ideas are exactly what RAG based Pdf assistants follow. a RAG assistant retrieves the top K chunk generates an answer from them and displays the supporting page evidence. this makes the system more transparent and when the system is transparent users tend to trust it more.

\begin{table}[t]
\caption{Summary of The Key Methods in Documents Question Answering}
\label{tab:literature_review_strict}
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.15}

\begin{tabularx}{\columnwidth}{|
>{\raggedright\arraybackslash}p{1.8cm}|
>{\raggedright\arraybackslash\hsize=.9\hsize}X|
>{\raggedright\arraybackslash\hsize=1.05\hsize}X|
>{\raggedright\arraybackslash\hsize=1.05\hsize}X|}
\hline

\textbf{Studys} & \textbf{Methods} & \textbf{Advantages} & \textbf{Limitations} \\

\hline
\cite{SaltonBuckley1988} & Sparse Retrieval TF IDF & Fast and Simple performs well on short queries and small data set & Limited semantics understanding weak for distributed informations \\
\hline
\cite{RobertsonZaragoza2009} & Sparse Retrieval BM25 & Efficient ranking using terms weighting & Lacks contextual semantic understanding \\
\hline
\cite{Vaswani2017} & Dense Retrieval Dpr & Semantic matchings beyond keyword & Higher latency embedding preprocessing required \\
\hline
\cite{Devlin2019} & Dense Retrieval FAISS & Fast largescale vectors search to with gpu acceleration & Embedding generation is computationally expensive \\
\hline
\cite{Johnson2017} & LLM Generation GPT & Fluent a natural language generations & Risk of hallucination and unsupported answes \\
\hline
\cite{Karpukhin2020} & LLM Generation BERT & Strong contextual embedding and understanding & Limited generative capability \\
\hline
\cite{Lewis2020RAG} & RAG System & Grounded outputs with retrieval support & Complex systems setup \\
\hline
\cite{Guu2020REALM} & RAG System & More accurate and verifiables response & Limited transparency \\
\hline
\cite{Borgeaud2022RETRO} & Multi Agent Approaches & Specialized agent improves performance & Higher coordination complexity \\
\hline
\cite{Asai2023SelfRAG} & Role Specialized Agens & Improved reliabilitys and output qualitys & Requires stricts to workflow design \\
\hline
\cite{Beltagy2020Longformer} & Retrievals Augmented Validation & Ensures compliances and grounded result & Depends on external documentation quality \\
\hline
\cite{Zaheer2020BigBird} & Iterative Critique and feedback & Reduces errors through iterative refinement & Needs structured monitoring \\
\hline
\cite{NogueiraCho2019} & Passage Reranking with BERT & Improves top ranked passage accuracy & Additional computational costs \\
\hline
\cite{KhattabZaharia2020ColBERT} & ColBERT & Good balance between effectiveness and speed & More complex architecture \\
\hline
\cite{Lewis2020BART} & BART & Strong summarization and generations performance & Large and resourceintensive \\
\hline
\cite{Zhang2020PEGASUS} & PEGASUS & Excellent abstractive summarization & Requires large scale pretraining \\
\hline
\cite{Lin2004ROUGE} & ROUGE Evaluation Metric & Standard automatics summarization evaluation & Limited semantic evaluation \\
\hline

\end{tabularx}
\end{table}


\section{Challenges}\label{sec:challenges}

There are a severall to challenges that are especially pertinents pdf based a assistant drivening by prior work and comon use expectation

Semantic mis matching in retrieval lexical technique can fail to locate relevants passage when the a query use a paraphrases or domain specific synonyms ~\cite{SaltonBuckley1988,RobertsonZaragoza2009}.

Latency constraints dense a retrieval improves recal but introduce embedding and the vector search overhead which might reduce interactive usability as a retrieval depth increases ~\cite{Johnson2017,Karpukhin2020}.

Hallucination under the incomplete context without properly retrieved evidence generations models might produce fluenting but unsupprted claim as which motivates evidence grounded generation ~\cite{Lewis2020RAG}.

Distributed evidence across pages: Many queries require cross section synthesis increases the need for chunking strategies top-$k$ retrieval that preserve context.

User trust and verification: Document assistant are more credible when they provide citations excerpts or highlighting that allow to users to verify the answer directly in the sources.

\section{Proposed Solution}\label{sec:proposed_solution}

DocuChat is a Pdf based RAG assistant that combine dense retrieval with evidence conditioned response generation in a lightweight web interface.The design objective is not only to provide answer but also to make the basis of each answer visible to the user.

Fig.~\ref{fig:architecture} illustrate the system pipeline a pdf is ingested and a converted into cleaned text. The document is segmented into overlapping chunk to reduce boundary effects when relevant information span adjacent paragraphs. Each chunk is embedded and stored in a FAISS vector index. At query time, the question is embedded into the same space the top$k$ chunks are retrieved and the retrieved text is passed to the LLM with a prompt that constrains the response to the provided context metadata is retained to support citation and highlight views.

\begin{figure}[t]

\centering

\includegraphics[width=1.1\linewidth,height=0.20\textheight]{architecture.jpg}

\caption{DocuChat system architecture (logo/watermark to be removed in camera ready diagram export).}

\label{fig:architecture}

\end{figure}

Fig.~\ref{fig:process} summarizes the interaction and inference flow from a user perspective and clarifies how retrieval is integrated into answer generation. the process begins when the user uploads documents and submits a naturallanguage questions through the UI system first processes the questions into an a embedding vector and performs similarity search over the FAISS index to obtain the top $k$ candidate chunks as evidence these retrieved chunks are then a assembled into a bounded context window, ordered and formatted for readability and injected into the Llm prompt so that generation is explicitly conditioned on the retrieved passages rather than in unsupported prior knowledge the Llm produces an answers that is grounded in this evidence context and DocuChat returns both the final response and the supporting chunks to the interface presenting evidence alongside the answer improves transparency and enables rapid verification while also making it easier to diagnose whether an incorrect output was caused by retrieval or generation This workflow therefore operationalizes DocuChat core design principle combining retrieval accuracy with userauditable grounding

 The process begins when the user uploads a document and submits a naturallanguage question through the UI the system first processes the question into an embedding vector and performs similarity search over the FAISS index to obtain the top $k$ candidate chunks as evidence. These retrieved chunks are then assembled into a bounded contexts window ordered and formatted for readability and injected into the Llm prompt so that generation is explicitly conditioned on the retrieved passages rather than on unsupported prior knowledge. The LLM produces an answer that is grounded in this evidence context and DocuChat returns both the final response and the supporting chunks to the interface.Presenting evidence alongside the answer improves transparency and enables rapid verification while also making it easier to diagnose whether an incorrect output was caused by retrieval or generation This workflow therefore operationalizes DocuChat core design principle combining retrieval accuracy with userauditable grounding.

\begin{figure}[!ht]

\centering

\includegraphics[width=1\linewidth]{process_flowchart.png}

\caption{DocuChat retrieval and generation workflow (logo/watermark to be removed in cameraready diagram export).}

\label{fig:process}

\end{figure}

\FloatBarrier

Fig.~\ref{fig:er} presents the data model used to relate documents chunk embedding and conversational artifacts supporting traceability and potentials persistence. Uploaded Pdfs are represented as document entities that connect to multiple chunk records generated to during preprocessing enabling an onetomany mapping from each source document to its segmented text units each chunk is linked to its corresponding embedding vectors representation which allows the system to retrieve candidates efficiently while still preserving the original chunk text for evidence display on the interaction side user queries and system answers are stored as conversation artifacts enabling sessionlevel history and reproducible evaluation runs crucially the model supports explicit linkage between a specific answers and the set of the chunks retrieved to produce it which enables citationstyle grounding auditing and debugging of system behavior.This relational structure is also suitable for future extensions such as tracking retrieval coverage statistics logging latency per stage storing user feedback on answer quality and maintaining persistent knowledge bases across multiple uploaded documents.

\begin{figure}[!ht]

\centering

\includegraphics[width=1\linewidth,height=0.25\textheight]{er_diagram.png}

\caption{Entity relationship model for document chunk embeddings querie and answers (logo to be removed in camera ready diagram export).}

\label{fig:er}

\end{figure}

\FloatBarrier

\section{Implementation}\label{sec:implementation}

The prototype is a deployed as a streamlit applicaition and it has modular pipeline. The pipeline include document ingestion preprocessing, indexing retrieval and response showing the system architecture keep separation between stages this make experimentation and future changes more easy. Each module work independent and communicate with structured data between steps because of this configuration can change without breaking whole system.

Pdf parsing is done with standard python tools. After parsing extracted text is normalize to a remove layout problems like broken lines repeated headers and footers page numbers and bad spacing other preprocessing include fixing whitespace removing non important symbols and trying keep structure when possible so meaning is not losted. These steps help better embedding quality and reduce noise from document format.

Chunking use to overlapping sliding windows with around 500 to 800 tokens per a chunk and 50 to 100 tokens overlap. This help balance between small pieces and keeping enough context. Overlap help reduce information loss and improve retrieval when question is the related to multiple parts of document.

Chunks is converted into to the dense vectors and indexed using a FAISS for the a fast similarity searching question embeddings are calculated in the same embedding space and top k nearest a neighbor retrieval is used evaluation test a  different k values to see retrieval sensitivity and answer quality. Retrieved passages are joined with separators and structured prompts before sending to the LLM prompt include clearing instruction to use just only one given context so hallucination and unsupported answers become less.

The evaluation process is done using different test queriesing to the check how the system perform in real scenarios a the questions include both simple keyword search and more descriptive natural languages querie results are observed manually to see if retrieved chunks are relevant and if generated answers stayin inside given context some basics comparison is done between different chunk sizes and overlap settings to understand how configuration affect retrievals quality of overall testing show that modular pipeline make debugging more easy and allow fast iteration when improving system components.

The user interface is designed simple and easy to use so users can be interact withing a system without complex setup the a application streamlit page show different stages of pipeline and allows uploading documents of the running indexing and asking questions in one place retrieved results are displayed together with sources information a so user can be understand where is the answer coming from this also help trust and make validation more easy during testing and development process.

Table~\ref{tab:tech_stack} the summarizes a core technologies used to by the prototype in addition to standard queryresponse interaction of the interface supports evidence inspection through contextual display and excerpt highlighting (Fig.~\ref{fig:context_highlighting}) this feature allows users to verify the provenance of the generated answers facilitating transparency and strengthening trust in the a system outputs. The designing emphasize explainability to by a exposing retrieved evidence directly within the users interface reinforcing verification a the primary interaction steps.

\begin{table}[h]

\caption{DocuChat Implementation Component}

\label{tab:tech_stack}

\centering

\scriptsize

\setlength{\tabcolsep}{4.2pt}

\renewcommand{\arraystretch}{1.15}

\begin{tabularx}{\columnwidth}{@{}p{0.28\columnwidth}p{0.26\columnwidth}X@{}}

\toprule

\textbf{Component} & \textbf{Technology} & \textbf{Role in System} \\

\midrule

Frontend & Streamlit & Provides upload UI chatstyles Q\&A, evidence display and export functionality. \\

Pdf Parsing & \texttt{PyPdf2} / \texttt{pdfplumber} & Handles text extraction and basics layout normalization. \\

Chunking & Overlapping token windows & Preserve local coherence and reduce boundary loss. \\

Embeddings & OpenAI embeddings & maps queries and chunks into a shared vector space \\

Vector Index & FAISS & Efficient similarity search over chunk embeddings \\

LLM answering & Openai GPT style model & Generates answers constrained by retrieveed context. \\

Persistence & SQLite & Stores documents chunks and query metadata for traceability. \\

\bottomrule

\end{tabularx}

\end{table}

\begin{figure}[!t]

\centering

\includegraphics[width=1.02\linewidth]{pdf_ingestion.png}

\caption{PDF ingestion view in the Streamlit interfaces.}

\label{fig:pdf_ingestion}

\end{figure}

\begin{figure}[!t]

\centering

\includegraphics[width=1.02\linewidth]{embedding_retrieval.png}

\caption{Embedding and retrieval interface for top $k$ evidence selection.}

\label{fig:embedding_retrieval}

\end{figure}

\begin{figure}[!t]

\centering

\includegraphics[width=1.03\linewidth]{answer_generation.png}

\caption{Answer generation view based on retrieved context.}

\label{fig:answer_generation}

\end{figure}

\section{Results}\label{sec:results}

The retrieval system demonstrate strong performances for concept search and definition querie achieving %100 coverage at k=3 k=3 k=3.This indicate is effective semantic matching for localized information multi page reasoning shows lower coverage %80 at k=5k=5k=5 suggesting that evidence distributed acros multiple document section remain challenging with limited retrieval depth.

Latency increases approximately linearly with top k. The mean response time rise from 1.2 seconds at k=1k=1k=1 to 5.0 seconds at k=10k=10k=10, reflecting the trade off between evidence coverage and responsivenes. For interactive scenarios moderate value of k=3–5k=3\text{–}5k=3–5 provide a practical balance.

User feedback is align with a these observation. Transparency received the highest rating (4.7/5) indicating the importance of evidence visibility. Latency received the lowest rating (4.2/5) consistent with measured response times.

These results suggest that in document centric workflows and auditability is prioritized over minimal latency.

Table~\ref{tab:retrieval_accuracy} shows the proportion of a case in which relevant chunks were retrieved for five query type. Performance remains high for concept search and definition (%100 at $k{=}3$). The lowest score occur for multipage reasoning (80\% at $k{=}5$), which align with the observation that distributed evidence is harder to capture with a limited number of retrieved chunks averaged over the five query types relevant-chunk coverage is 91\%, indicating that the dense retrieval configuration is effective for the tested scenarios while still leaving room for improvement on crosssection questions

\begin{table}[!t]

\caption{Retrieval Accuracy Across Query Types}

\label{tab:retrieval_accuracy}

\centering

\scriptsize

\setlength{\tabcolsep}{4.5pt}

\renewcommand{\arraystretch}{1.15}

\begin{tabular}{@{}l l c c@{}}

\toprule

\textbf{Query ID} & \textbf{Query Type} & \textbf{Relevant Chunks Found (\%)} & \textbf{Top $k$} \\

\midrule

Q01 & Concept search & 100 & 3 \\

Q02 & Fact look up & 90 & 3 \\

Q03 & Multipage reasoning & 80 & 5 \\

Q04 & Definition & 100 & 3 \\

Q05 & Procedure & 85 & 5 \\

\bottomrule

\end{tabular}

\end{table}

 two form of user feedback are reported. Table~\ref{tab:user_quotes} lists qualitative comment from four participant which emphasize evidence visibility and clarity. Table~\ref{tab:user_feedback_scores} summarize quantitative score on a 1-5 scale with transparency (4.7) and ease of use (4.6) rated highest notably latency receive the lowest of the four aggregate scores (4.2) aligning with the measured growth in response time a retrieval depth increase. Fig.~\ref{fig:user_feedback} visualize these rating.

\begin{table}[!t]

\caption{User Feedbacks (Qualitative Comments)}

\label{tab:user_quotes}

\centering

\footnotesize

\setlength{\tabcolsep}{4pt}

\renewcommand{\arraystretch}{1.15}

\begin{tabularx}{\columnwidth}{@{}p{0.18\columnwidth}Y@{}}

\toprule

\textbf{Participant} & \textbf{Feedback} \\

\midrule

P1 & ``Im really trust the result becuse the sources are shown.'' \\

P2 & ``Response are fast and summaries are very clear i recomend!'' \\

P3 & ``Simple interface chunk adjustment are effective and interesting'' \\

P4 & ``Transparency is high; citations improve trust.'' \\

\bottomrule

\end{tabularx}

\end{table}

\begin{table}[!t]

\caption{User Feedback Summary (1--5 Scale)}

\label{tab:user_feedback_scores}

\centering

\scriptsize

\setlength{\tabcolsep}{5.0pt}

\renewcommand{\arraystretch}{1.15}

\begin{tabular}{@{}l c p{0.48\columnwidth}@{}}

\toprule

\textbf{Metric} & \textbf{Score} & \textbf{Comment} \\

\midrule

Answer accuracy & 4.5 & Answers perceived as precise and grounded \\

Transparency & 4.7 & Page level citation and evidence display improved trust \\

Ease of use & 4.6 & Interface perceived as simple and intuitive \\

Latency & 4.2 & Response time acceptable for small Pdfs increases with larger contexts \\

\bottomrule

\end{tabular}

\end{table}

\begin{figure}[!t]

\centering

\includegraphics[width=0.95\linewidth, height=0.30\textheight]{user_feedback.png}

\caption{Graphical summary of user feedback.}

\label{fig:user_feedback}

\end{figure}

Fig.~\ref{fig:context_highlighting} illustrate the evidence oriented interaction design by linking an answer to highlighted excerpt in the source Pdf.This feature address a central usability concern in LLM based assistants: even when an answer is correct user frequently need to confirm it in context, especially for technical definitions and constraints in DocuChat highlighting provides a direct path from generated text to documentary evidence,strengthening the systems auditability.

\begin{figure}[t]

\centering

\includegraphics[width=1.02\linewidth]{context_highlighting.png}

\caption{Example of excerpt display and highlighting within the Pdf view.}

\label{fig:context_highlighting}

\end{figure}

Table~\ref{tab:latency} and Fig.~\ref{fig:latency_graph} show that querys latency increases with retrieval depths. The mean latency grows from 1.2\,s at $k{=}1$ to 2.4\,s at $k{=}3$, reaching 5.0\,seconds at $k{=}10$ this behavior reflects an expected trade-off: larger $k$ increases the amount of retrieved evidence and the prompt context passed to the generator, improving recall for distributed evidence at the cost of responsivenes for interactive use, the results suggest that moderate $k$ values offer a reasonable compromise between evidence coverage and perceived speed.

\begin{table}[!t]

\caption{Query Latency as a Function of Retrieval Depth}

\label{tab:latency}

\centering

\scriptsize

\setlength{\tabcolsep}{5.0pt}

\renewcommand{\arraystretch}{1.15}

\begin{tabular}{@{}c c c l@{}}

\toprule

\textbf{Top-$k$} & \textbf{Avg.\ Latency (s)} & \textbf{Std.\ Dev.} & \textbf{Comment} \\

\midrule

1 & 1.2 & 0.3 & Fast for short question \\

3 & 2.4 & 0.5 & Typical interactive setting \\

5 & 3.1 & 0.7 & Slower improve coverage on longer Pdf \\

10 & 5.0 & 1.2 & Heavy context noticeable delay \\

\bottomrule

\end{tabular}

\end{table}

\begin{figure}[!t]

\centering

\includegraphics[width=1.02\linewidth]{latency_graph.png}

\caption{Latency trend as retrieval depth (top$k$) increases.}

\label{fig:latency_graph}

\end{figure}

\section{Future Work}\label{sec:future_work}

Local improvements is could be increase robustness and privacy. Multidocument retrieval would enable cross document question answering a hybrid sparse dense pipeline can improve recall under vocabulary mismatch while lightweight crossencoder re ranking can be raise precision when top-$k$ contains semantically close but irrelevant chunks replacing cloud embeddings and generation with local models would reduce API costs and the strengthen privacy guarantees evidence features could add sentencelevel citations clickable page links and an a evidence panels listing the exact chunks used larger stratified evaluations with relevance labels would better test generalization.

\section{Conclusion}\label{sec:conclusion}

DocuChat is a RAG assistant that extracts text chunks it embeds content and retrieves passages via FAISS to generate evidencegrounded answers a Streamlit UI shows context and highlights source excerpt experiments report 80-100 \% relevant chunk coverage across five query types (mean 91\%). users praised transparency and usability but higher top k increased latency. Limitations is include a small scope unclear relevance labels and reliance on the external API motivating hybrid retrieval reranking and local models.
\clearpage

\section*{Acknowledgment}

All author helped write the paper and approve the final version there was no funding the author have no conflicts of interest the code and implementation resources are available on GitHub repositories listed inproject material.

\begin{enumerate}

\item \textbf{Funding:} Not applicable.

\item \textbf{Conflict of Interest:} None.

\item \textbf{Ethics Approval and Consent to Participate:} Not applicable.

\item \textbf{Consent for Publication:} All author have provided consent.

\item \textbf{Data Availability:} Prototype evaluation artifact include a demonstration repository and an informals feedback summary; rawsurvey responses are not publiclys released.

\item \textbf{Code Availability:} The sourcecode is available at

\url{https://github.com/cuneyted} and

\url{https://github.com/doruk3535}.

\end{enumerate}

\begin{thebibliography}{00}

\bibitem{SaltonBuckley1988}

G.~Salton and C.~Buckley, ``Term-weighting approaches in automatic text retrieval,'' \emph{Information Processing \& Management}, vol.~24, no.~5, pp.~513--523, 1988.

\bibitem{RobertsonZaragoza2009}

S.~Robertson and H.~Zaragoza, ``The probabilistic relevance framework: BM25 and beyond,'' \emph{Foundations and Trends in Information Retrieval}, vol.~3, no.~4, pp.~333--389, 2009.

\bibitem{Vaswani2017}

A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N.~Gomez, {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' in \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem{Devlin2019}

J.~Devlin, M.-W.~Chang, K.~Lee, and K.~Toutanova, ``BERT: Pre-training of deep bidirectional transformers for language understanding,'' in \emph{Proc. NAACL-HLT}, 2019, pp.~4171--4186.

\bibitem{Johnson2017}

J.~Johnson, M.~Douze, and H.~J\'egou, ``Billion-scale similarity search with GPUs,'' \emph{arXiv preprint} arXiv:1702.08734, 2017.

\bibitem{Karpukhin2020}

V.~Karpukhin, B.~O\u{g}uz, S.~Min, P.~Lewis, L.~Wu, S.~Edunov, D.~Chen, and W.-t.~Yih, ``Dense passage retrieval for open-domain question answering,'' in \emph{Proc. EMNLP}, 2020, pp.~6769--6781.

\bibitem{Lewis2020RAG}

P.~Lewis, E.~Perez, A.~Piktus, F.~Petroni, V.~Karpukhin, N.~Goyal, H.~K\"uttler, M.~Lewis, W.-t.~Yih, T.~Rockt\"aschel, S.~Riedel, and D.~Kiela, ``Retrieval-augmented generation for knowledge-intensive NLP tasks,'' in \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{Guu2020REALM}

K.~Guu, K.~Lee, Z.~Tung, P.~Pasupat, and M.-W.~Chang, ``REALM: Retrieval-augmented language model pre-training,'' in \emph{Proc. Int. Conf. Machine Learning (ICML)}, 2020.

\bibitem{Borgeaud2022RETRO}

S.~Borgeaud, A.~Mensch, J.~Hoffmann, T.~Cai, E.~Rutherford, K.~Millican, G.~van~den~Driessche, J.-B.~Lespiau, B.~Damoc, A.~Clark, \emph{et al.}, ``Improving language models by retrieving from trillions of tokens,'' in \emph{Proc. ICML}, 2022, pp.~2206--2240.

\bibitem{Asai2023SelfRAG}

A.~Asai, Z.~Wu, Y.~Wang, A.~Sil, and H.~Hajishirzi, ``Self-RAG: Learning to retrieve, generate, and critique through self-reflection,'' \emph{arXiv preprint} arXiv:2310.11511, 2023.

\bibitem{Beltagy2020Longformer}

I.~Beltagy, M.~E.~Peters, and A.~Cohan, ``Longformer: The long-document transformer,'' \emph{arXiv preprint} arXiv:2004.05150, 2020.

\bibitem{Zaheer2020BigBird}

M.~Zaheer, G.~Guruganesh, A.~Dubey, J.~Ainslie, C.~Alberti, S.~Ontanon, P.~Pham, A.~Ravula, Q.~Wang, L.~Yang, and A.~Ahmed, ``Big Bird: Transformers for longer sequences,'' in \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{NogueiraCho2019}

R.~Nogueira and K.~Cho, ``Passage re-ranking with BERT,'' \emph{arXiv preprint} arXiv:1901.04085, 2019.

\bibitem{KhattabZaharia2020ColBERT}

O.~Khattab and M.~Zaharia, ``ColBERT: Efficient and effective passage search via contextualized late interaction over BERT,'' in \emph{Proc. 43rd Int. ACM SIGIR Conf. Research and Development in Information Retrieval (SIGIR)}, 2020.

\bibitem{Lewis2020BART}

M.~Lewis, Y.~Liu, N.~Goyal, M.~Ghazvininejad, A.~Mohamed, O.~Levy, V.~Stoyanov, and L.~Zettlemoyer, ``BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,'' in \emph{Proc. ACL}, 2020, pp.~7871--7880.

\bibitem{Zhang2020PEGASUS}

J.~Zhang, Y.~Zhao, M.~Saleh, and P.~Liu, ``PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization,'' in \emph{Proc. ICML}, 2020, pp.~11328--11339.

\bibitem{Lin2004ROUGE}

C.-Y.~Lin, ``ROUGE: A package for automatic evaluation of summaries,'' in \emph{Text Summarization Branches Out}, 2004, pp.~74--81.

\end{thebibliography}

\end{document}
